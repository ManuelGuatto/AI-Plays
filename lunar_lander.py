# -*- coding: utf-8 -*-
"""Lunar_Lander.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FXyMGH3V9s1tzl2--Ktx-uVGo-P_BchC

# Lunar Landing

The aim of this notebook is to exploit different learning techniques:

*   Deep Q-Learning
*   PPO

## Import the environment and the video functions
In this section we will import the environment and define some function to render the simulations
"""

!pip3 install box2d-py
!pip install pygame
!pip3 install gym[box_2D]
import gym
env = gym.make("LunarLander-v2")

# install dependencies needed for recording videos
!apt-get install -y xvfb x11-utils
!pip install pyvirtualdisplay==0.2.*

from pyvirtualdisplay import Display
display = Display(visible=False, size=(1400, 900))
_ = display.start()

from gym.wrappers.monitoring.video_recorder import VideoRecorder
before_training = "before_training.mp4"

video = VideoRecorder(env, before_training)
# returns an initial observation
env.reset()
for i in range(200):
  env.render()
  video.capture_frame()
  # env.action_space.sample() produces either 0 (left) or 1 (right).
  observation, reward, done, info = env.step(env.action_space.sample())
  # Not printing this time
  #print("step", i, observation, reward, done, info)

video.close()
env.close()

from base64 import b64encode
def render_mp4(videopath: str) -> str:
  """
  Gets a string containing a b4-encoded version of the MP4 video
  at the specified path.
  """
  mp4 = open(videopath, 'rb').read()
  base64_encoded_mp4 = b64encode(mp4).decode()
  return f'<video width=400 controls><source src="data:video/mp4;' \
         f'base64,{base64_encoded_mp4}" type="video/mp4"></video>'

from IPython.display import HTML
html = render_mp4(before_training)
HTML(html)

"""## Deep Q-Learning

### Define Network Architecture
"""

from torch import nn
import torch
import random
import numpy as np
from tqdm.notebook import tqdm
from collections import deque
import matplotlib.pyplot as plt

class QNN(nn.Module):
  def __init__(self,state_space_dim, action_space_dim, hid_neur_1, hid_neur_2):
    super().__init__()

    self.linear = nn.Sequential(
            nn.Linear(state_space_dim, hid_neur_1),
            nn.Tanh(),
            nn.Linear(hid_neur_1, hid_neur_2),
            nn.Tanh(),
            nn.Linear(hid_neur_2, action_space_dim)

            )
    
  def forward(self, x):
    return self.linear(x)

class ReplayMemory:
  def __init__(self, dimension):
    self.memory = deque(maxlen=dimension)
  
  def add(self, state, action, next_state, reward):
    self.memory.append((state, action, next_state, reward))

  def __len__(self):
        return len(self.memory)

  def sample(self, batch_size):
        batch_size = min(batch_size, len(self)) 
        return random.sample(self.memory, batch_size)

"""###Train the agent"""

def softmax_action_selection(net, state, temperature):
    
    if temperature < 0:
        raise Exception('The temperature value must be greater than or equal to 0 ')
    
    # Evaluate the network output from the current state
    with torch.no_grad():
        net.eval()
        state = torch.tensor(state, dtype=torch.float32)
        net_out = net(state)

    if temperature == 0:
      return int(net_out.argmax()), net_out.numpy()

    # Apply softmax with temp
    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability
    softmax_out = nn.functional.softmax(net_out / temperature, dim=0).numpy()
                
    # Sample the action using softmax output as mass pdf
    all_possible_actions = np.arange(0, softmax_out.shape[-1])
    action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from "all_possible_actions" with the probability distribution p (softmax_out in this case)
    
    return action, net_out.numpy()

def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):
        
    # Sample the data from the replay memory
    batch = replay_mem.sample(batch_size)
    batch_size = len(batch)

    # Create tensors for each element of the batch
    states      = torch.tensor([s[0] for s in batch], dtype=torch.float32)
    actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)
    rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32)

    # Compute a mask of non-final states (all the elements where the next state is not None)
    non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32) # the next state can be None if the game has ended
    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)

    # Compute all the Q values (forward pass)
    policy_net.train()
    q_values = policy_net(states)
    # Select the proper Q value for the corresponding action taken Q(s_t, a)
    state_action_values = q_values.gather(1, actions.unsqueeze(1))

    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )
    with torch.no_grad():
      target_net.eval()
      q_values_target = target_net(non_final_next_states)
    next_state_max_q_values = torch.zeros(batch_size)
    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]

    # Compute the expected Q values
    expected_state_action_values = rewards + (next_state_max_q_values * gamma)
    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape

    # Compute the Huber loss
    loss = loss_fn(state_action_values, expected_state_action_values)

    # Optimize the model
    optimizer.zero_grad()
    loss.backward()
    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)
    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)
    optimizer.step()

# Set random seeds
torch.manual_seed(0)
np.random.seed(0)
random.seed(0)

### PARAMETERS
state_space_dim = env.observation_space.shape[0]
action_space_dim = env.action_space.n
hid_neur_1 = 128
hid_neur_2 = 64
gamma = 0.99   # gamma parameter for the long term reward
replay_memory_capacity = 10000   # Replay memory capacity
lr = 1e-3   # Optimizer learning rate
target_net_update_steps = 10   # Number of episodes to wait before updating the target network
batch_size = 512   # Number of samples to take from the replay memory for each update
bad_state_penalty = 0   # Penalty to the reward when we are in a bad state (in this case when the pole falls down) 
min_samples_for_training = 1000   # Minimum samples in the replay memory to enable the training

### Initialize the replay memory
replay_mem = ReplayMemory(replay_memory_capacity)    

### Initialize the policy network
policy_net = QNN(state_space_dim, action_space_dim, hid_neur_1, hid_neur_2)

### Initialize the target network with the same weights of the policy network
target_net = QNN(state_space_dim, action_space_dim, hid_neur_1, hid_neur_2)
target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network

### Initialize the optimizer
optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network

### Initialize the loss function (Huber loss)
loss_fn = nn.MSELoss()

### Define exploration profile
initial_value = 3
num_iterations = 1000
exp_decay = np.exp(-np.log(initial_value) / num_iterations * 2) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations
exploration_profile = [initial_value * (exp_decay ** i) for i in range(num_iterations)]

### Plot exploration profile
plt.figure(figsize=(12,8))
plt.plot(exploration_profile)
plt.grid()
plt.xlabel('Iteration')
plt.ylabel('Exploration profile (Softmax temperature)')

score_view = 100
env.seed(0) # Set a random seed for the environment (reproducible results)
scoring_list = np.zeros(score_view)
it = 0

for episode_num, tau in enumerate(tqdm(exploration_profile)):

    # Reset the environment and get the initial state
    state = env.reset()
    # Reset the score. The final score will be the total amount of steps before the pole falls
    score = 0
    done = False

    # Go on until the pole falls off
    while not done:

      # Choose the action following the policy
      action, q_values = softmax_action_selection(policy_net, state, temperature=tau)
      
      # Apply the action and get the next state, the reward and a flag "done" that is True if the game is ended
      next_state, reward, done, info = env.step(action)

      # Update the final score (+1 for each step)
      score += reward

      #Compute new reward
      pos_weight = 10
      velocity_weight = 5
      reward = reward - pos_weight * np.sqrt((state[0])**2+(state[1])**2+(state[3])**2)

      # Apply penalty for bad state
      if done: # if the pole has fallen down 
          reward += bad_state_penalty
          next_state = None

      # Update the replay memory
      replay_mem.add(state, action, next_state, reward)

      # Update the network
      if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often
          update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)

      # Visually render the environment (disable to speed up the training)
      #env.render()

      # Set the current state for the next iteration
      state = next_state

    #Save the score
    scoring_list[it] = score
    it = it+1
    # Update the target network every target_net_update_steps episodes
    if episode_num % target_net_update_steps == 0:
        #print('Updating target network...')
        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network

    # Print the final score
    if (episode_num+1)%100 == 0:
      #print(f"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}") # Print the final score
      print(f"EPISODE: {episode_num + 1} - FINAL SCORE: {np.sum(scoring_list)/score_view} - Temperature: {tau}") # Print the average score
      it = 0
      scoring_list = np.zeros(score_view)

env.close()

"""###Test the agent"""

env = gym.make("LunarLander-v2")
after_training_dq = "after_training_dq.mp4"
video = VideoRecorder(env, after_training_dq)
# Let's try for a total of 10 episodes
for num_episode in range(2): 
    # Reset the environment and get the initial state
    state = env.reset()
    # Reset the score. The final score will be the total amount of steps before the pole falls
    score = 0
    done = False
    # Go on until the pole falls off or the score reach 490
    while not done:
        # Choose the best action (temperature 0)
        action, q_values = softmax_action_selection(policy_net, state, temperature=0)
        # Apply the action and get the next state, the reward and a flag "done" that is True if the game is ended
        next_state, reward, done, info = env.step(action)
        # Visually render the environment
        env.render()
        video.capture_frame()
        # Update the final score (+1 for each step)
        score += reward
        # Set the current state for the next iteration
        state = next_state
        # Check if the episode ended (the pole fell down)
    # Print the final score
    print(f"EPISODE {num_episode + 1} - FINAL SCORE: {score}") 
video.close()
env.close()

html = render_mp4(after_training_dq)
HTML(html)

"""##PPO

We will proceed to implement the PPO versione using the actor critic technique. For this purpose we will define two ANN.

###Define Actor Critic network architecture
"""

from torch import nn
import torch
import random
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
import torch.nn.functional as F
import torch.distributions as distributions

class MLP_prob(nn.Module):
  def __init__(self, input_dim, output_dim, hid_neur_1, hid_neur_2, dropout = 0.1):
    super().__init__()

    self.linear = nn.Sequential(
            nn.Linear(input_dim, hid_neur_1),
            nn.Dropout(dropout),
            nn.ReLU(),
            nn.Linear(hid_neur_1, hid_neur_2),
            nn.ReLU(),
            nn.Linear(hid_neur_2, output_dim),
            nn.Softmax()
            )
    
  def forward(self, x):
    return self.linear(x)

class MLP(nn.Module):
  def __init__(self, input_dim, output_dim, hid_neur_1, hid_neur_2, dropout = 0.1):
    super().__init__()

    self.linear = nn.Sequential(
            nn.Linear(input_dim, hid_neur_1),
            nn.Dropout(dropout),
            nn.ReLU(),
            nn.Linear(hid_neur_1, hid_neur_2),
            nn.ReLU(),
            nn.Linear(hid_neur_2, output_dim),
            )
    
  def forward(self, x):
    return self.linear(x)

class ActorCritic(nn.Module):
  def __init__(self, actor, critic):
    super().__init__()

    self.actor = actor
    self.critic = critic

  def forward(self, state):
    action_prob = self.actor(state)
    state_value = self.critic(state)
    return action_prob, state_value

"""###Define the parameters"""

INPUT_DIM = env.observation_space.shape[0]
OUTPUT_DIM = env.action_space.n
HIDDEN_DIM_1 = 256
HIDDEN_DIM_2 = 64
LEARNING_RATE = 1e-3

actor = MLP_prob(INPUT_DIM, OUTPUT_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2)
critic = MLP(INPUT_DIM, 1, HIDDEN_DIM_1, HIDDEN_DIM_2)

actor_critic = ActorCritic(actor, critic)
optim = torch.optim.Adam(actor_critic.parameters(), lr = LEARNING_RATE)

def init_weights(m):
    if type(m) == nn.Linear:
        torch.nn.init.xavier_normal_(m.weight)
        m.bias.data.fill_(0)

actor_critic.apply(init_weights)

"""###Training procedure"""

def train (env, network, optimizer, discount_factor, ppo_steps, ppo_clip):
  network.train()
        
  states = []
  actions = []
  log_prob_actions = []
  values = []
  rewards = []
  done = False
  episode_reward = 0

  state = env.reset()

  while not done:

        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

        states.append(state)
        
        action_prob, value_pred = network(state)
                
        policy = distributions.Categorical(action_prob)
        
        action = policy.sample()

        log_prob_action = policy.log_prob(action)
        
        state, reward, done, _ = env.step(action.item())

        actions.append(action)
        log_prob_actions.append(log_prob_action)
        values.append(value_pred)
        rewards.append(reward)
        
        episode_reward += reward
    
  states = torch.cat(states)
  actions = torch.cat(actions)    
  log_prob_actions = torch.cat(log_prob_actions)
  values = torch.cat(values).squeeze(-1)
   
  returns = calculate_returns(rewards, discount_factor)
  advantages = calculate_advantages(returns, values)
    
  policy_loss, value_loss = update_policy(network, states, actions, log_prob_actions, advantages, returns, optimizer, ppo_steps, ppo_clip)

  return policy_loss, value_loss, episode_reward

def calculate_returns(rewards, discount_factor, normalize = True):
    
    returns = []
    R = 0
    
    for r in reversed(rewards):
        R = r + R * discount_factor
        returns.insert(0, R)
        
    returns = torch.tensor(returns)
    
    if normalize:
        returns = (returns - returns.mean()) / returns.std()
        
    return returns

def calculate_advantages(returns, values, normalize = True):
    
    advantages = returns - values
    
    if normalize:
        
        advantages = (advantages - advantages.mean()) / advantages.std()
        
    return advantages

def update_policy(network, states, actions, log_prob_actions, advantages, returns, optimizer, ppo_steps, ppo_clip):
    
    total_policy_loss = 0 
    total_value_loss = 0
    
    states = states.detach()
    actions = actions.detach()
    log_prob_actions = log_prob_actions.detach()
    advantages = advantages.detach()
    returns = returns.detach()
    
    for _ in range(ppo_steps):
                
        action_prob, value_pred = network(states)
        value_pred = value_pred.squeeze(-1)
        
        policy = distributions.Categorical(action_prob)
        
        new_log_prob_actions = policy.log_prob(actions)
        
        policy_ratio = (new_log_prob_actions - log_prob_actions).exp()
                
        loss_1 = policy_ratio * advantages
        loss_2 = torch.clamp(policy_ratio, min = 1.0 - ppo_clip, max = 1.0 + ppo_clip) * advantages
        
        policy_loss = - torch.min(loss_1, loss_2).mean()
        
        value_loss = F.smooth_l1_loss(returns, value_pred).mean()
    
        optimizer.zero_grad()

        policy_loss.backward()
        value_loss.backward()

        optimizer.step()
    
        total_policy_loss += policy_loss.item()
        total_value_loss += value_loss.item()
    
    return total_policy_loss / ppo_steps, total_value_loss / ppo_steps

def evaluate(env, network):
    
    network.eval()
    
    rewards = []
    done = False
    episode_reward = 0

    state = env.reset()

    while not done:

        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

        with torch.no_grad():
        
            action_prob, _ = network(state)
                
        action = torch.argmax(action_prob, dim = -1)
                
        state, reward, done, _ = env.step(action.item())

        episode_reward += reward
        
    return episode_reward

MAX_EPISODES = 1_000
DISCOUNT_FACTOR = 0.99
PPO_STEPS = 5
PPO_CLIP = 0.2
N_TRIALS = 25
REWARD_THRESHOLD = 200
PRINT_STEPS = 10
MAX_TIME_THRESHOLD_CROSSED = 5


train_env = gym.make('LunarLander-v2')
test_env = gym.make('LunarLander-v2')

train_rewards = []
test_rewards = []

cross_threshold_counter = 0


for episode in range(1, MAX_EPISODES+1):
    
    _, _, train_reward = train(train_env, actor_critic, optim, DISCOUNT_FACTOR, PPO_STEPS, PPO_CLIP)
    
    test_reward = evaluate(test_env, actor_critic)
    
    train_rewards.append(train_reward)
    test_rewards.append(test_reward)
    
    mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])
    mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])
    
    if episode % PRINT_STEPS == 0:
        
        print(f' EPISODE: {episode:3} - MEAN TRAIN REWARDS: {mean_train_rewards:7.1f} - MEAN TEST REWARDS: {mean_test_rewards:7.1f} |')
    
    if (mean_test_rewards >= REWARD_THRESHOLD):
        
        print(f'Reached reward threshold in {episode} episodes')

        cross_threshold_counter += 1

        if cross_threshold_counter == MAX_TIME_THRESHOLD_CROSSED:
        
          break

    else:
         cross_threshold_counter = 0

plt.figure(figsize=(12,8))
plt.plot(test_rewards, label='Test Reward')
plt.plot(train_rewards, label='Train Reward')
plt.xlabel('Episode', fontsize=20)
plt.ylabel('Reward', fontsize=20)
plt.hlines(REWARD_THRESHOLD, 0, len(test_rewards), color='r')
plt.legend(loc='lower right')
plt.grid()

"""###Final Test"""

env = env = gym.make("LunarLander-v2")
after_training = "after_training.mp4"
video = VideoRecorder(env, after_training)

actor_critic.eval()
    
rewards = []
done = False
episode_reward = 0

state = env.reset()

while not done:
  state = torch.FloatTensor(state).unsqueeze(0)

  with torch.no_grad():
        
    action_prob, _ = actor_critic(state)
                
  action = torch.argmax(action_prob, dim = -1)
                
  state, reward, done, _ = env.step(action.item())

  env.render()
  video.capture_frame()

  episode_reward += reward

video.close()
env.close()
print(episode_reward)

html = render_mp4(after_training)
HTML(html)