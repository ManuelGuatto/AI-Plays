# -*- coding: utf-8 -*-
"""CartPole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lv6DwBvK_f491csp7FkvF4z3SBmK_5hp

#CartPole problem
"""

!pip3 install box2d-py
!pip install pygame
!pip3 install gym[box_2D]
import gym

# install dependencies needed for recording videos
!apt-get install -y xvfb x11-utils
!pip install pyvirtualdisplay==0.2.*

import glob
import io
import pygame
import base64
import os
from IPython.display import HTML
from IPython import display as ipythondisplay

import random
import torch
import numpy as np
import gym
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

from torch import nn
from collections import deque # this python module implements exactly what we need for the replay memeory

"""# Enable gym environment rendering in Colab

> This section is not required if you are executing the notebook in a local environment. It enables the video generation from Gym environments, and it is only required in Colab since we do not have access to a screen.

Source: https://colab.research.google.com/drive/18LdlDDT87eb8cCTHZsXyS9ksQPzL3i6H
"""

from pyvirtualdisplay import Display
display = Display(visible=False, size=(1400, 900))
_ = display.start()

!pip install gym[classic_control]

from gym.wrappers.monitoring.video_recorder import VideoRecorder
before_training = "before_training.mp4"
env = gym.make('CartPole-v1', render_mode='human') 
env = gym.wrappers.RecordVideo(env,'video', episode_trigger = lambda x: x % 2 == 0)
video = VideoRecorder(env, before_training)
env.reset()
# returns an initial observation
for i in range(200):
  env.render()
  video.capture_frame()
  # env.action_space.sample() produces either 0 (left) or 1 (right).
  observation, reward, done, info = env.step(env.action_space.sample())
  # Not printing this time
  #print("step", i, observation, reward, done, info)

video.close()
env.close()

from base64 import b64encode
def render_mp4(videopath: str) -> str:
  """
  Gets a string containing a b4-encoded version of the MP4 video
  at the specified path.
  """
  mp4 = open(videopath, 'rb').read()
  base64_encoded_mp4 = b64encode(mp4).decode()
  return f'<video width=400 controls><source src="data:video/mp4;' \
         f'base64,{base64_encoded_mp4}" type="video/mp4"></video>'

from IPython.display import HTML
html = render_mp4(before_training)
HTML(html)

"""# Experience replay (Replay Memory)"""

class ReplayMemory(object):

    def __init__(self, capacity):
        self.memory = deque(maxlen=capacity) # Define a queue with maxlen "capacity"

    def push(self, state, action, next_state, reward):
        # TODO: Add the tuple (state, action, next_state, reward) to the queue
        self.memory.append((state, action, next_state, reward))

    def sample(self, batch_size):
        batch_size = min(batch_size, len(self)) # Get all the samples if the requested batch_size is higher than the number of sample currently in the memory
        return random.sample(self.memory, batch_size) # Randomly select "batch_size" samples

    def __len__(self):
        return len(self.memory) # Return the number of samples currently stored in the memory

"""Test if it works as expected."""

# Define the replay memory
replay_mem = ReplayMemory(capacity=3)

# Push some samples
print(f"CURRENT MEMORY SIZE: {len(replay_mem)}")
replay_mem.push(1,1,1,1)
print(f"CURRENT MEMORY SIZE: {len(replay_mem)}")
replay_mem.push(2,2,2,2)
print(f"CURRENT MEMORY SIZE: {len(replay_mem)}")
replay_mem.push(3,3,3,3)
print(f"CURRENT MEMORY SIZE: {len(replay_mem)}")
replay_mem.push(4,4,4,4)
print(f"CURRENT MEMORY SIZE: {len(replay_mem)}")
replay_mem.push(5,5,5,5)
print(f"CURRENT MEMORY SIZE: {len(replay_mem)}")

# Check the content of the memory
print('\nCONTENT OF THE MEMORY')
print(replay_mem.memory)

# Random sample
print('\nRANDOM SAMPLING')
for i in range(5):
  print(replay_mem.sample(2)) # Select 2 samples randomly from the memory

"""# Policy network

## Network definition
"""

class DQN(nn.Module):

    def __init__(self, state_space_dim, action_space_dim):
        super().__init__()

        self.linear = nn.Sequential(
            nn.Linear(state_space_dim, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.Tanh(),
            nn.Linear(128, action_space_dim)

                )

    def forward(self, x):
        return self.linear(x)

# Define an example network
net = DQN(state_space_dim=4, action_space_dim=2)

"""## Exploration Policy

Starting from the estimated Q-values (one for each action), we need to choose the proper action. This action may be the one expected to provide the highest long term reward (exploitation), or maybe we want to find a better policy by choosing a different action (exploration).

The exploration policy controls this behavior, typically by varying a single parameter.

Since our Q-values estimates are far from the true values at the beginning of the training, a high exploration is preferred in the initial phase.

The steps are:

`Current state -> Policy network -> Q-values -> Exploration Policy -> Action`

### Epsilon-greedy policy

With an epsilon-greedy policy we choose a **non optimal** action with probability epsilon, otherwise choose the best action (the one corresponding to the highest Q-value).

NOTE: there is a difference wrt paper in that they do not exclude the best action when chosing the action at random
"""

def choose_action_epsilon_greedy(net, state, epsilon):
    
    if epsilon > 1 or epsilon < 0:
        raise Exception('The epsilon value must be between 0 and 1')
                
    # Evaluate the network output from the current state
    with torch.no_grad():
        net.eval()
        state = torch.tensor(state, dtype=torch.float32) # Convert the state to tensor
        net_out = net(state)

    # Get the best action (argmax of the network output)
    best_action = int(net_out.argmax())
    # Get the number of possible actions
    action_space_dim = net_out.shape[-1]

    # Select a non optimal action with probability epsilon, otherwise choose the best action
    if random.random() < epsilon:
        # List of non-optimal actions
        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]
        # Select randomly
        action = random.choice(non_optimal_actions)
    else:
        # Select best action
        action = best_action
        
    return action, net_out.numpy()

# Test if it works as expected
state = (0, 0, 0, 0)
epsilon = 0.5
chosen_action, q_values = choose_action_epsilon_greedy(net, state, epsilon)

print(f"ACTION: {chosen_action}")
print(f"Q-VALUES: {q_values}")

"""### Softmax policy

With a softmax policy we choose the action based on a distribution obtained applying a softmax (with temperature $\tau$) to the estimated Q-values. The highest the temperature, the more the distribution will converge to a random uniform distribution. At zero temperature, instead, the policy will always choose the action with the highest Q-value.

> **HINT**
>
> To sample from a random (discrete) distribution you can use the numpy function `numpy.random.choice` (https://docs.scipy.org/doc//numpy-1.10.4/reference/generated/numpy.random.choice.html), by setting the parameter `p` properly.

> **HINT**
>
> Even if the PyTorch softmax function does not support the temperature parameter directly, you can still use it and apply the temperature outside the function. Just look carefully at the softmax formula in the slide above.

> **HINT**
>
> The softmax function may be numerically unstable with very low temperature values. In practice, it is suggested to set a minimum value for the temperature (e.g. 1e-8).
"""

def choose_action_softmax(net, state, temperature):
    
    if temperature < 0:
        raise Exception('The temperature value must be greater than or equal to 0 ')
        
    # If the temperature is 0, just select the best action using the eps-greedy policy with epsilon = 0
    if temperature == 0:
        return choose_action_epsilon_greedy(net, state, 0)
    
    # Evaluate the network output from the current state
    with torch.no_grad():
        net.eval()
        state = torch.tensor(state, dtype=torch.float32)
        net_out = net(state)

    # Apply softmax with temp
    temperature = max(temperature, 1e-8) # set a minimum to the temperature for numerical stability
    softmax_out = nn.functional.softmax(net_out / temperature, dim=0).numpy()
                
    # Sample the action using softmax output as mass pdf
    all_possible_actions = np.arange(0, softmax_out.shape[-1])
    action = np.random.choice(all_possible_actions, p=softmax_out) # this samples a random element from "all_possible_actions" with the probability distribution p (softmax_out in this case)
    
    return action, net_out.numpy()

state = (0, 0, 0, 0)
temperature = 1
chosen_action, q_values = choose_action_softmax(net, state, temperature)

print(f"ACTION: {chosen_action}")
print(f"Q-VALUES: {q_values}")

"""### Exploration profile

Let's consider, for example, an exponentially decreasing exploration profile using a softmax policy.

$$
\text{softmax_temperature}  = \text{initial_temperature} * \text{exp_decay}^i \qquad \text{for $i$ = 1, 2, ..., num_iterations } 
$$

Alternatively, you can consider an epsilon greedy policy. In this case the exploration would be controlled by the epsilon parameter, for which you should consider a different initial value (max 1).
"""

### Define exploration profile
initial_value = 5
num_iterations = 1000
exp_decay = np.exp(-np.log(initial_value) / num_iterations * 2) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations
first_exploration_profile = [initial_value * (exp_decay ** i) for i in range(num_iterations)]

"""# Gym Environment (CartPole-v1)

A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.

https://gym.openai.com/envs/CartPole-v1/

![image.png](data:image/png;base64,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)
"""

### Create environment
env = gym.make('CartPole-v1') # Initialize the Gym environment
env.seed(0) # Set a random seed for the environment (reproducible results)

# Get the shapes of the state space (observation_space) and action space (action_space)
state_space_dim = env.observation_space.shape[0]
action_space_dim = env.action_space.n

print(f"STATE SPACE SIZE: {state_space_dim}")
print(f"ACTION SPACE SIZE: {action_space_dim}")

"""# Network update

## Initialization

In this case we will use the Huber loss as loss function (https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html). The Huber loss uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It is less sensitive to outliers than the MSELoss and in some cases prevents exploding gradients.
"""

# Set random seeds
torch.manual_seed(0)
np.random.seed(0)
random.seed(0)

### PARAMETERS
gamma = 0.97   # gamma parameter for the long term reward
replay_memory_capacity = 10000   # Replay memory capacity
lr = 1e-2   # Optimizer learning rate
target_net_update_steps = 10   # Number of episodes to wait before updating the target network
batch_size = 128   # Number of samples to take from the replay memory for each update
bad_state_penalty = 0   # Penalty to the reward when we are in a bad state (in this case when the pole falls down) 
min_samples_for_training = 1000   # Minimum samples in the replay memory to enable the training

### Initialize the replay memory
replay_mem = ReplayMemory(replay_memory_capacity)    

### Initialize the policy network
policy_net = DQN(state_space_dim, action_space_dim)

### Initialize the target network with the same weights of the policy network
target_net = DQN(state_space_dim, action_space_dim)
target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network

### Initialize the optimizer
optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network

### Initialize the loss function (Huber loss)
loss_fn = nn.SmoothL1Loss()

"""## Update function"""

def update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size):
        
    # Sample the data from the replay memory
    batch = replay_mem.sample(batch_size)
    batch_size = len(batch)

    # Create tensors for each element of the batch
    states      = torch.tensor([s[0] for s in batch], dtype=torch.float32)
    actions     = torch.tensor([s[1] for s in batch], dtype=torch.int64)
    rewards     = torch.tensor([s[3] for s in batch], dtype=torch.float32)

    # Compute a mask of non-final states (all the elements where the next state is not None)
    non_final_next_states = torch.tensor([s[2] for s in batch if s[2] is not None], dtype=torch.float32) # the next state can be None if the game has ended
    non_final_mask = torch.tensor([s[2] is not None for s in batch], dtype=torch.bool)

    # Compute all the Q values (forward pass)
    policy_net.train()
    q_values = policy_net(states)
    # Select the proper Q value for the corresponding action taken Q(s_t, a)
    state_action_values = q_values.gather(1, actions.unsqueeze(1))

    # Compute the value function of the next states using the target network V(s_{t+1}) = max_a( Q_target(s_{t+1}, a)) )
    with torch.no_grad():
      target_net.eval()
      q_values_target = target_net(non_final_next_states)
    next_state_max_q_values = torch.zeros(batch_size)
    next_state_max_q_values[non_final_mask] = q_values_target.max(dim=1)[0]

    # Compute the expected Q values
    expected_state_action_values = rewards + (next_state_max_q_values * gamma)
    expected_state_action_values = expected_state_action_values.unsqueeze(1) # Set the required tensor shape

    # Compute the Huber loss
    loss = loss_fn(state_action_values, expected_state_action_values)

    # Optimize the model
    optimizer.zero_grad()
    loss.backward()
    # Apply gradient clipping (clip all the gradients greater than 2 for training stability)
    nn.utils.clip_grad_norm_(policy_net.parameters(), 2)
    optimizer.step()

### Define exploration profile
initial_value = 5
num_iterations = 350
exp_decay = np.exp(-np.log(initial_value) / num_iterations * 6) # We compute the exponential decay in such a way the shape of the exploration profile does not depend on the number of iterations
exploration_profile = [initial_value * (exp_decay ** i) for i in range(num_iterations)]

### Plot exploration profile
plt.figure(figsize=(12,8))
plt.plot(first_exploration_profile, label = 'Exploration profile with more exploration')
plt.plot(exploration_profile, label = 'First Exploration profile')
plt.legend()
plt.grid()
plt.xlabel('Iteration')
plt.ylabel('Exploration profile (Softmax temperature)')

"""## Training loop

"""

torch.manual_seed(0)

### Initialize the replay memory
replay_mem = ReplayMemory(replay_memory_capacity)    

### Initialize the policy network
policy_net = DQN(state_space_dim, action_space_dim)

### Initialize the target network with the same weights of the policy network
target_net = DQN(state_space_dim, action_space_dim)
target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network

### Initialize the optimizer
optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network

### Initialize the loss function (Huber loss)
loss_fn = nn.SmoothL1Loss()

# Initialize the Gym environment
env = gym.make('CartPole-v1') 
env.seed(0) # Set a random seed for the environment (reproducible results)

# This is for creating the output video in Colab, not required outside Colab
#env = wrap_env(env, video_callable=lambda episode_id: episode_id % 100 == 0) # Save a video every 100 episodes
score_list_normed_angle = []

for episode_num, tau in enumerate(tqdm(exploration_profile)):

    # Reset the environment and get the initial state
    state = env.reset()
    # Reset the score. The final score will be the total amount of steps before the pole falls
    score = 0
    alpha = 10
    done = False

    # Go on until the pole falls off
    while not done:

      # Choose the action following the policy
      action, q_values = choose_action_softmax(policy_net, state, temperature=tau)
      
      # Apply the action and get the next state, the reward and a flag "done" that is True if the game is ended
      next_state, reward, done, info = env.step(action)


      pos_weight = 1
      # We apply a (linear) penalty when the cart is far from center
      reward =  reward + - pos_weight * np.abs(state[0]) - alpha*(abs(next_state[2]))

      # Update the final score (+1 for each step)
      score += 1

      # Apply penalty for bad state
      if done: # if the pole has fallen down 
          reward += bad_state_penalty
          next_state = None

      # Update the replay memory
      replay_mem.push(state, action, next_state, reward)

      # Update the network
      if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often
          update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)

      # Visually render the environment (disable to speed up the training)
      #env.render()

      # Set the current state for the next iteration
      state = next_state

    #Save the score
    score_list_normed_angle.append(score)

    # Update the target network every target_net_update_steps episodes
    if episode_num % target_net_update_steps == 0:
        print('Updating target network...')
        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network

    # Print the final score
    print(f"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}") # Print the final score

env.close()

torch.manual_seed(0)

### Initialize the replay memory
replay_mem = ReplayMemory(replay_memory_capacity)    

### Initialize the policy network
policy_net = DQN(state_space_dim, action_space_dim)

### Initialize the target network with the same weights of the policy network
target_net = DQN(state_space_dim, action_space_dim)
target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network

### Initialize the optimizer
optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network

### Initialize the loss function (Huber loss)
loss_fn = nn.SmoothL1Loss()

# Initialize the Gym environment
env = gym.make('CartPole-v1') 
env.seed(0) # Set a random seed for the environment (reproducible results)

max_position = 4.8
max_angle = 0.418

# This is for creating the output video in Colab, not required outside Colab
#env = wrap_env(env, video_callable=lambda episode_id: episode_id % 100 == 0) # Save a video every 100 episodes
score_list_normalized_v = []

for episode_num, tau in enumerate(tqdm(exploration_profile)):

    # Reset the environment and get the initial state
    state = env.reset()
    # Reset the score. The final score will be the total amount of steps before the pole falls
    score = 0
    alpha = 100
    done = False

    # Go on until the pole falls off
    while not done:

      # Choose the action following the policy
      action, q_values = choose_action_softmax(policy_net, state, temperature=tau)
      
      # Apply the action and get the next state, the reward and a flag "done" that is True if the game is ended
      next_state, reward, done, info = env.step(action)

      #We give a reward taking care about the angle displacement from the equilibrium and the displacement
      #of the box from the center
      reward = reward - (abs(state[0]/max_position) + abs(state[2]/max_angle))
      # Update the final score (+1 for each step)
      score += 1

      # Apply penalty for bad state
      if done: # if the pole has fallen down 
          reward += bad_state_penalty
          next_state = None

      # Update the replay memory
      replay_mem.push(state, action, next_state, reward)

      # Update the network
      if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often
          update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)

      # Visually render the environment (disable to speed up the training)
      #env.render()

      # Set the current state for the next iteration
      state = next_state

    #Save the score
    score_list_normalized_v.append(score)

    # Update the target network every target_net_update_steps episodes
    if episode_num % target_net_update_steps == 0:
        print('Updating target network...')
        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network

    # Print the final score
    print(f"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}") # Print the final score

env.close()

# Display the videos, not required outside Colab
#show_videos()

torch.manual_seed(0)

### Initialize the replay memory
replay_mem = ReplayMemory(replay_memory_capacity)    

### Initialize the policy network
policy_net = DQN(state_space_dim, action_space_dim)

### Initialize the target network with the same weights of the policy network
target_net = DQN(state_space_dim, action_space_dim)
target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network

### Initialize the optimizer
optimizer = torch.optim.SGD(policy_net.parameters(), lr=lr) # The optimizer will update ONLY the parameters of the policy network

### Initialize the loss function (Huber loss)
loss_fn = nn.SmoothL1Loss()

# Initialize the Gym environment
env = gym.make('CartPole-v1') 
env.seed(0) # Set a random seed for the environment (reproducible results)

# This is for creating the output video in Colab, not required outside Colab
#env = wrap_env(env, video_callable=lambda episode_id: episode_id % 100 == 0) # Save a video every 100 episodes
score_list_l_v = []

for episode_num, tau in enumerate(tqdm(exploration_profile)):

    # Reset the environment and get the initial state
    state = env.reset()
    # Reset the score. The final score will be the total amount of steps before the pole falls
    score = 0
    alpha = 100
    done = False

    # Go on until the pole falls off
    while not done:

      # Choose the action following the policy
      action, q_values = choose_action_softmax(policy_net, state, temperature=tau)
      
      # Apply the action and get the next state, the reward and a flag "done" that is True if the game is ended
      next_state, reward, done, info = env.step(action)

      #We give a reward taking care about the angle displacement from the equilibrium and the displacement
      #of the box from the center
      reward = -alpha*(abs(next_state[2])-abs(state[2]))
      # Update the final score (+1 for each step)
      score += 1

      # Apply penalty for bad state
      if done: # if the pole has fallen down 
          reward += bad_state_penalty
          next_state = None

      # Update the replay memory
      replay_mem.push(state, action, next_state, reward)

      # Update the network
      if len(replay_mem) > min_samples_for_training: # we enable the training only if we have enough samples in the replay memory, otherwise the training will use the same samples too often
          update_step(policy_net, target_net, replay_mem, gamma, optimizer, loss_fn, batch_size)

      # Visually render the environment (disable to speed up the training)
      #env.render()

      # Set the current state for the next iteration
      state = next_state

    #Save the score
    score_list_l_v.append(score)

    # Update the target network every target_net_update_steps episodes
    if episode_num % target_net_update_steps == 0:
        print('Updating target network...')
        target_net.load_state_dict(policy_net.state_dict()) # This will copy the weights of the policy network to the target network

    # Print the final score
    print(f"EPISODE: {episode_num + 1} - FINAL SCORE: {score} - Temperature: {tau}") # Print the final score

env.close()

# Plot losses of train and validation
plt.figure(figsize=(24,16))
plt.plot(score_list_normed_angle, label='Normed Angle Reward Score')
plt.plot(score_list_normalized_v, label='Reward Normalized distance and Angle Score')
plt.plot(score_list_l_v, label='Displacement Angle Reward Score')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.grid()
plt.title('Score with different rewards')
plt.legend()
plt.show()
plt.savefig('loss_plot_Standard.png', bbox_inches='tight')

"""# Final test"""

from pyvirtualdisplay import Display
display = Display(visible=False, size=(1400, 900))
_ = display.start()

# Initialize the Gym environment
test = "test.mp4"
env = gym.make('CartPole-v1', render_mode='human') 
env = gym.wrappers.RecordVideo(env,'video', episode_trigger = lambda x: x % 2 == 0)
video = VideoRecorder(env, test)

# Let's try for a total of 1 episode
for num_episode in range(1): 
    # Reset the environment and get the initial state
    state = env.reset()
    # Reset the score. The final score will be the total amount of steps before the pole falls
    score = 0
    done = False
    # Go on until the pole falls off or the score reach 490
    while not done:
      # Choose the best action (temperature 0)
      action, q_values = choose_action_softmax(policy_net, state, temperature=0)
      # Apply the action and get the next state, the reward and a flag "done" that is True if the game is ended
      next_state, reward, done, info = env.step(action)
      # Visually render the environment
      env.render()
      video.capture_frame()
      # Update the final score (+1 for each step)
      score += reward 
      # Set the current state for the next iteration
      state = next_state
      # Check if the episode ended (the pole fell down)
    # Print the final score
    print(f"EPISODE {num_episode + 1} - FINAL SCORE: {score}") 
env.close()
video.close()

from IPython.display import HTML
html = render_mp4(test)
HTML(html)